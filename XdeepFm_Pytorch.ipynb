{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiqN5xuriSrvEON+4dHA5m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IkramKheopsys/eXtreme-Deep-Factorization-Machine-xDeepFM-/blob/main/XdeepFm_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LOVv45gY9y1P"
      },
      "outputs": [],
      "source": [
        "# Tencent is pleased to support the open source community by making Angel available.\n",
        "#\n",
        "# Copyright (C) 2017-2018 THL A29 Limited, a Tencent company. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in\n",
        "# compliance with the License. You may obtain a copy of the License at\n",
        "#\n",
        "# https://opensource.org/licenses/Apache-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software distributed under the License\n",
        "# is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express\n",
        "# or implied. See the License for the specific language governing permissions and limitations under\n",
        "# the License.\n",
        "#\n",
        "# !/usr/bin/env python\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class xDeepFM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim=-1, n_fields=-1, embedding_dim=-1, fc_dims=[], cin_dims=[], encode=\"onehot\"):\n",
        "        super(xDeepFM, self).__init__()\n",
        "        self.loss_fn = torch.nn.BCELoss()\n",
        "        self.input_dim = input_dim\n",
        "        self.n_fields = n_fields\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.fc_dims = fc_dims\n",
        "        self.cin_dims = cin_dims\n",
        "        self.encode = encode\n",
        "        self.mats = []\n",
        "        # local model do not need real input_dim to init params, so set fake_dim to\n",
        "        # speed up to produce local pt file.\n",
        "        fake_input_dim = 10\n",
        "        if input_dim > 0 and n_fields > 0 and embedding_dim > 0 and fc_dims and cin_dims:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(1, 1))\n",
        "            self.weights = torch.nn.Parameter(torch.zeros(fake_input_dim, 1))\n",
        "            self.embedding = torch.nn.Parameter(torch.zeros(fake_input_dim, embedding_dim))\n",
        "            torch.nn.init.xavier_uniform_(self.weights)\n",
        "            torch.nn.init.xavier_uniform_(self.embedding)\n",
        "            # cin\n",
        "            n_fields_list = [n_fields]\n",
        "            for i, size in enumerate(self.cin_dims):\n",
        "                w = torch.nn.init.kaiming_uniform_(torch.nn.Parameter(\n",
        "                    torch.zeros(size, n_fields_list[-1] * n_fields_list[0], 1)), mode='fan_in', nonlinearity='relu')\n",
        "                b = torch.nn.Parameter(torch.zeros(size, 1))\n",
        "                self.mats.append(w.view(size, n_fields_list[-1] * n_fields_list[0]))\n",
        "                self.mats.append(b)\n",
        "                n_fields_list.append(size)\n",
        "            # mlps\n",
        "            dim = n_fields * embedding_dim\n",
        "            for (index, fc_dim) in enumerate(fc_dims):\n",
        "                self.mats.append(torch.nn.Parameter(torch.randn(dim, fc_dim)))\n",
        "                self.mats.append(torch.nn.Parameter(torch.zeros(1, 1)))\n",
        "                torch.nn.init.kaiming_uniform_(self.mats[index * 2], mode='fan_in', nonlinearity='relu')\n",
        "                dim = fc_dim\n",
        "            w = torch.nn.Parameter(torch.nn.init.kaiming_uniform_(torch.zeros(fc_dims[-1] + sum(cin_dims), 1),\n",
        "                                                                  mode='fan_in', nonlinearity='relu'))\n",
        "            self.mats.append(w.view(-1, 1))\n",
        "            self.mats.append(torch.nn.Parameter(torch.zeros(1, 1)))\n",
        "\n",
        "    def first_order(self, batch_size, index, values, bias, weights):\n",
        "        # type: (int, Tensor, Tensor, Tensor, Tensor) -> Tensor\n",
        "        size = batch_size\n",
        "        srcs = weights.view(1, -1).mul(values.view(1, -1)).view(-1)\n",
        "        output = torch.zeros(size, dtype=torch.float32)\n",
        "        output.scatter_add_(0, index, srcs)\n",
        "        first = output + bias\n",
        "        return first\n",
        "\n",
        "    def cin(self, batch_size, index, embeddings, mats, fields):\n",
        "        # type: (int, Tensor, Tensor, List[Tensor], Tensor) -> Tensor\n",
        "        b = batch_size\n",
        "        n_fields = self.n_fields\n",
        "        embedding_dim = embeddings.size(1)\n",
        "        if self.encode == \"onehot\":\n",
        "            x0 = embeddings.view(b, n_fields, embedding_dim)\n",
        "        else:\n",
        "            t_index = [index, fields]\n",
        "            e_transpose = embeddings.view(-1, embedding_dim).transpose(0, 1)\n",
        "            count = torch.ones(embeddings.size(0))\n",
        "            hs = []\n",
        "            for i in range(embedding_dim):\n",
        "                h = torch.zeros(b, n_fields)\n",
        "                c = torch.zeros(b, n_fields)\n",
        "                h.index_put_(t_index, e_transpose[i], True)\n",
        "                c.index_put_(t_index, count, True)\n",
        "                h = h / c.clamp(min=1)\n",
        "                hs.append(h.view(-1, 1))\n",
        "            emb_cat = torch.cat(hs, dim=1)\n",
        "            x0 = emb_cat.view(batch_size, n_fields, embedding_dim)\n",
        "        results = []\n",
        "        xk = x0\n",
        "        for i in range(len(self.cin_dims)):\n",
        "            z = torch.einsum('bhd,bmd->bhmd', xk, x0)\n",
        "            z = z.view(b, xk.shape[1] * n_fields, embedding_dim) # b * hk * d\n",
        "            filter_w = mats[i * 2].view(self.cin_dims[i], xk.shape[1] * n_fields, 1)\n",
        "            filter_b = mats[i * 2 + 1].view(self.cin_dims[i])\n",
        "            x_out = F.conv1d(z, filter_w, filter_b)\n",
        "            x_out = torch.relu(x_out)\n",
        "            next_hidden, res = x_out, x_out\n",
        "            xk = next_hidden\n",
        "            results.append(res)\n",
        "        final_result = torch.cat(results, dim=1)\n",
        "        final_result = torch.sum(final_result, dim=2)\n",
        "        return final_result.view(b, -1)\n",
        "\n",
        "    def deep(self, batch_size, index, embeddings, mats, fields):\n",
        "        # type: (int, Tensor, Tensor, List[Tensor], Tensor) -> Tensor\n",
        "        if self.encode == \"onehot\":\n",
        "            b = batch_size\n",
        "            output = embeddings.view(b, -1)\n",
        "        else:\n",
        "            k = embeddings.size(1)\n",
        "            b = batch_size\n",
        "            f = self.n_fields\n",
        "            t_index = [index, fields]\n",
        "            e_transpose = embeddings.view(-1, k).transpose(0, 1)\n",
        "            count = torch.ones(embeddings.size(0))\n",
        "            hs = []\n",
        "            for i in range(k):\n",
        "                h = torch.zeros(b, f)\n",
        "                c = torch.zeros(b, f)\n",
        "                h.index_put_(t_index, e_transpose[i], True)\n",
        "                c.index_put_(t_index, count, True)\n",
        "                h = h / c.clamp(min=1)\n",
        "                hs.append(h.view(-1, 1))\n",
        "            emb_cat = torch.cat(hs, dim=1)\n",
        "            output = emb_cat.view(batch_size, -1)\n",
        "\n",
        "        for i in range(int(len(mats) / 2)):\n",
        "            output = torch.relu(output.matmul(mats[i * 2]) + mats[i * 2 + 1])\n",
        "\n",
        "        return output.view(b, -1)  # [b * 1]\n",
        "\n",
        "    def forward_(self, batch_size, index, feats, values, bias, weights, embeddings, mats, fields=torch.Tensor([])):\n",
        "        # type: (int, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, List[Tensor], Tensor) -> Tensor\n",
        "        first = self.first_order(batch_size, index, values, bias, weights)\n",
        "        cin_index = len(self.cin_dims) * 2\n",
        "        cin = self.cin(batch_size, index, embeddings, mats[0:cin_index], fields)\n",
        "        deep = self.deep(batch_size, index, embeddings, mats[cin_index:-2], fields)\n",
        "        cin_and_deep = torch.cat([cin, deep], dim=1)\n",
        "        output = torch.matmul(cin_and_deep, mats[-2]).view(-1)\n",
        "        output = output + first + mats[-1].view(-1)\n",
        "\n",
        "        return torch.sigmoid(output)\n",
        "\n",
        "    def forward(self, batch_size, index, feats, values, fields=torch.Tensor([])):\n",
        "        # type: (int, Tensor, Tensor, Tensor, Tensor) -> Tensor\n",
        "        batch_first = F.embedding(feats, self.weights)\n",
        "        emb = F.embedding(feats, self.embedding)\n",
        "        return self.forward_(batch_size, index, feats, values,\n",
        "                             self.bias, batch_first, emb, self.mats, fields)\n",
        "\n",
        "    @torch.jit.export\n",
        "    def loss(self, output, targets):\n",
        "        return self.loss_fn(output, targets)\n",
        "\n",
        "    @torch.jit.export\n",
        "    def get_type(self):\n",
        "        if self.encode == \"onehot\":\n",
        "            return \"BIAS_WEIGHT_EMBEDDING_MATS\"\n",
        "        else:\n",
        "            return \"BIAS_WEIGHT_EMBEDDING_MATS_FIELD\"\n",
        "\n",
        "    @torch.jit.export\n",
        "    def get_name(self):\n",
        "        return \"xDeepFM\"\n",
        "\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "\n",
        "def main():\n",
        "    xdeepfm = xDeepFM(FLAGS.input_dim, FLAGS.n_fields, FLAGS.embedding_dim, FLAGS.fc_dims, FLAGS.cin_dims, FLAGS.encode)\n",
        "    #xdeepfm_script_module = torch.jit.script(xdeepfm)\n",
        "    #xdeepfm_script_module.save(\"xdeepfm.pt\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
        "    parser.add_argument(\n",
        "        \"--input_dim\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"data input dim.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--n_fields\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"data num fields.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--encode\",\n",
        "        type=str,\n",
        "        default=\"onehot\",\n",
        "        help=\"data encode.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--embedding_dim\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"embedding dim.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fc_dims\",\n",
        "        nargs=\"+\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"fc layers dim list.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cin_dims\",\n",
        "        nargs=\"+\",\n",
        "        type=int,\n",
        "        default=-1,\n",
        "        help=\"cin layers dim list.\"\n",
        "    )\n",
        "    FLAGS, unparsed = parser.parse_known_args()\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"movielens_sample.txt\")\n",
        "sparse_features = [\"movie_id\", \"user_id\",\n",
        "                    \"gender\", \"age\", \"occupation\", \"zip\"]\n",
        "target = ['rating']\n",
        "\n",
        "# 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
        "for feat in sparse_features:\n",
        "    lbe = LabelEncoder()\n",
        "    data[feat] = lbe.fit_transform(data[feat])\n",
        "# 2.count #unique features for each sparse field\n",
        "fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique())\n",
        "                          for feat in sparse_features]\n",
        "linear_feature_columns = fixlen_feature_columns\n",
        "dnn_feature_columns = fixlen_feature_columns\n",
        "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
        "\n",
        "# 3.generate input data for model\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "train_model_input = {name: train[name] for name in feature_names}\n",
        "test_model_input = {name: test[name] for name in feature_names}\n"
      ],
      "metadata": {
        "id": "0dYELpwZDJyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}